# üöÄ Pipeline de Datos MITMA + INE - Modelo de Gravedad

Pipeline de ingesta, transformaci√≥n y an√°lisis de datos de movilidad (MITMA) e indicadores socioecon√≥micos (INE) para la identificaci√≥n de d√©ficits de infraestructura de transporte en Espa√±a mediante un modelo de gravedad.

## üìë √çndice

- [Arquitectura General](#-arquitectura-general)
- [Fuentes de Datos (DataSources)](#-fuentes-de-datos-datasources)
- [Arquitectura Medallion (Bronze ‚Üí Silver ‚Üí Gold)](#-arquitectura-medallion-bronze--silver--gold)
- [Infraestructura Tecnol√≥gica](#-infraestructura-tecnol√≥gica)
- [DAGs de Airflow](#-dags-de-airflow)
- [Tablas y Transformaciones](#-tablas-y-transformaciones)
- [Sistema de Tracking de Procesamiento](#-sistema-de-tracking-de-procesamiento)
- [Google Cloud Run Jobs](#-google-cloud-run-jobs)
- [Modelo de Gravedad (Gold Layer)](#-modelo-de-gravedad-gold-layer)

---

## üèó Arquitectura General

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                              FUENTES EXTERNAS                                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ      MITMA RSS       ‚îÇ       INE API        ‚îÇ        Python Library             ‚îÇ
‚îÇ  (movilidad.mitma)   ‚îÇ   (servicios.ine)    ‚îÇ       (holidays)                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                              AIRFLOW DAGS                                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  bronze_mitma   ‚îÇ  ‚îÇ   bronze_ine    ‚îÇ  ‚îÇ       bronze_holidays           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  (OD, People,   ‚îÇ  ‚îÇ  (Municipios,   ‚îÇ  ‚îÇ   (Festivos nacionales)         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Overnights,   ‚îÇ  ‚îÇ   Empresas,     ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ  ‚îÇ   Zonification) ‚îÇ  ‚îÇ   Poblaci√≥n,    ‚îÇ                                       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ   Renta)        ‚îÇ                                       ‚îÇ
‚îÇ                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                    ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚ñº                               ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         RustFS S3             ‚îÇ    ‚îÇ            DuckLake                       ‚îÇ
‚îÇ   (Almacenamiento temporal)   ‚îÇ    ‚îÇ   (PostgreSQL metadata + S3 data files)  ‚îÇ
‚îÇ   - mitma-raw bucket          ‚îÇ    ‚îÇ   - Tablas Bronze, Silver, Gold          ‚îÇ
‚îÇ   - Archivos .csv.gz          ‚îÇ    ‚îÇ   - Particionado por fecha               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                                     ‚îÇ
                                                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           GOOGLE CLOUD RUN JOBS                                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ   ingestor_cloud            ‚îÇ  ‚îÇ        executor_cloud                    ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   (MERGE CSV ‚Üí DuckLake)    ‚îÇ  ‚îÇ   (Ejecuta SQL arbitrario)               ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ   - 32GB RAM, 8 CPUs        ‚îÇ  ‚îÇ   - Transformaciones Silver/Gold         ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìä Fuentes de Datos (DataSources)

### 1. MITMA (Ministerio de Transportes, Movilidad y Agenda Urbana)

**Origen:** RSS Feed `https://movilidad-opendata.mitma.es/RSS.xml`

El portal de datos abiertos de movilidad del MITMA proporciona datos de movilidad basados en datos de telefon√≠a m√≥vil anonimizados.

| Dataset | Descripci√≥n | Formato | Frecuencia |
|---------|-------------|---------|------------|
| **OD (Viajes)** | Matrices Origen-Destino con n¬∫ de viajes entre zonas | CSV.GZ | Diario |
| **People Day** | Personas por d√≠a seg√∫n zona de pernoctaci√≥n | CSV.GZ | Diario |
| **Overnight Stay** | Pernoctaciones por zona residencia/pernoctaci√≥n | CSV.GZ | Diario |
| **Zonification** | Geometr√≠as + metadatos de zonas (SHP + CSV) | SHP/CSV | Est√°tico |

**Tipos de zonificaci√≥n soportados:**
- `municipios`: Zonas a nivel municipal (~8,131 zonas)
- `distritos`: Zonas a nivel de distrito censal
- `gau`: Grandes √Åreas Urbanas

**Ejemplo de URL de datos OD:**
```
https://movilidad-opendata.mitma.es/estudios_basicos/por-municipios/viajes/ficheros-diarios/2023-03/20230306_Viajes_municipios.csv.gz
```

### 2. INE (Instituto Nacional de Estad√≠stica)

**Origen:** API JSON `https://servicios.ine.es/wstempus/js/ES/`

| Dataset | URL Base | Descripci√≥n |
|---------|----------|-------------|
| **Municipios** | `VALORES_VARIABLE/19` | Cat√°logo de municipios con c√≥digos INE |
| **Empresas** | `DATOS_TABLA/3955?tip=AM` | N¬∫ de empresas por municipio y a√±o |
| **Poblaci√≥n** | `DATOS_TABLA/2852?tip=AM` | Poblaci√≥n por municipio, sexo y a√±o |
| **Renta** | `DATOS_TABLA/31097?tip=AM` | Renta per c√°pita por municipio |

**Ejemplo de respuesta JSON (Municipios):**
```json
[
  {
    "Id": 28079,
    "Codigo": "28079",
    "Nombre": "Madrid"
  },
  ...
]
```

### 3. MITMA-INE Relations

**Origen:** `https://movilidad-opendata.mitma.es/` (CSV)

Tabla de correspondencia entre c√≥digos MITMA y c√≥digos INE para poder cruzar ambas fuentes de datos.

| Campo | Descripci√≥n |
|-------|-------------|
| `municipio_mitma` | ID de zona MITMA |
| `municipio_ine` | C√≥digo INE del municipio |

### 4. Festivos Espa√±oles (Holidays)

**Origen:** Librer√≠a Python `holidays`

Se genera program√°ticamente la lista de festivos nacionales espa√±oles para el a√±o solicitado. Se usa para enriquecer los datos de movilidad con flags `is_holiday`.

---

## üèÖ Arquitectura Medallion (Bronze ‚Üí Silver ‚Üí Gold)

### Bronze Layer (Raw Data)
- Datos en formato original (CSV/JSON)
- Schema flexible (`all_varchar = true`)
- Metadatos de auditor√≠a (`loaded_at`, `source_file`/`source_url`)
- Sin transformaciones de negocio

### Silver Layer (Cleaned & Enriched)
- Tipos de datos correctos (TIMESTAMP, DOUBLE, etc.)
- Filtros de calidad (nulls, valores inv√°lidos)
- Enriquecimiento con flags calculados
- Deduplicaci√≥n
- Normalizaci√≥n de nombres e IDs

### Gold Layer (Analytics)
- Tablas optimizadas para an√°lisis
- M√©tricas agregadas
- Modelo de gravedad calibrado
- Exports para visualizaci√≥n

---

## üîß Infraestructura Tecnol√≥gica

### Stack Principal

| Componente | Tecnolog√≠a | Prop√≥sito |
|------------|------------|-----------|
| **Orquestador** | Apache Airflow 3.x | Scheduling y gesti√≥n de DAGs |
| **Data Lakehouse** | DuckLake | Motor anal√≠tico sobre S3 con metadata en PostgreSQL |
| **Object Storage** | RustFS (S3-compatible) | Almacenamiento de archivos de datos |
| **Metadata Store** | PostgreSQL | Cat√°logo de tablas DuckLake |
| **Compute (Cloud)** | Google Cloud Run Jobs | Ejecuci√≥n de cargas pesadas |

### Conexiones de Airflow

```yaml
# Conexi√≥n S3 (RustFS)
rustfs_s3_conn:
  conn_type: aws
  extra:
    endpoint_url: http://rustfs:9000
    aws_access_key_id: admin
    aws_secret_access_key: ********

# Conexi√≥n PostgreSQL (DuckLake metadata)
postgres_datos_externos:
  conn_type: postgres
  host: postgres
  port: 5432
  schema: ducklake_db
  login: airflow
  password: ********

# Conexi√≥n Google Cloud (para Cloud Run)
google_cloud_default:
  conn_type: google_cloud_platform
  project_id: muceim-bigdata
  keyfile_json: {...}
```

### Variables de Airflow

```yaml
RUSTFS_BUCKET: "mitma"
GCP_PROJECT_ID: "muceim-bigdata"
GCP_CLOUD_RUN_REGION: "europe-southwest1"
GCP_CLOUD_RUN_JOB_NAME: "insert-ducklake"
GCP_CLOUD_RUN_EXECUTOR_JOB_NAME: "ducklake-executor"
```

---

## üìã DAGs de Airflow

### 1. `bronze_mitma` - Ingesta de datos MITMA

**Schedule:** Manual (`schedule=None`)  
**Tags:** `bronze`, `mitma`, `data-ingestion`

**Par√°metros:**
| Param | Tipo | Descripci√≥n |
|-------|------|-------------|
| `start` | string | Fecha inicio (YYYY-MM-DD) |
| `end` | string | Fecha fin (YYYY-MM-DD) |
| `enable_people_day` | boolean | Habilitar ingesta People Day |
| `enable_overnight` | boolean | Habilitar ingesta Overnight |

**TaskGroups:**
```
infra ‚îÄ‚ñ∫ od_municipios ‚îÄ‚ñ∫ done
     ‚îú‚îÄ‚ñ∫ people_day_municipios ‚îÄ‚ñ∫ done
     ‚îú‚îÄ‚ñ∫ overnight_municipios ‚îÄ‚ñ∫ done
     ‚îî‚îÄ‚ñ∫ zonification ‚îÄ‚ñ∫ done
```

**Flujo de cada TaskGroup (ej: OD):**
1. `od_urls`: Obtiene URLs del RSS para el rango de fechas
2. `od_filter_urls`: Filtra URLs ya procesadas (consulta `source_file` en tabla)
3. `check_urls`: Branch - si hay URLs procesa, si no salta
4. `od_create`: Crea tabla Bronze si no existe
5. `od_process`: **Dynamic Task Mapping** - procesa cada URL en paralelo:
   - Descarga archivo desde MITMA
   - Sube a RustFS (bucket `mitma-raw`)
   - Ejecuta Cloud Run Job para MERGE en DuckLake
   - Elimina archivo temporal de RustFS

### 2. `bronze_ine` - Ingesta de datos INE

**Schedule:** Manual (`schedule=None`)  
**Tags:** `bronze`, `ine`, `data-ingestion`

**Par√°metros:**
| Param | Tipo | Descripci√≥n |
|-------|------|-------------|
| `year` | string | A√±o de datos (YYYY) |

**TaskGroups:**
```
infra ‚îÄ‚ñ∫ municipios ‚îÄ‚ñ∫ done
     ‚îú‚îÄ‚ñ∫ empresas ‚îÄ‚ñ∫ done
     ‚îú‚îÄ‚ñ∫ poblacion ‚îÄ‚ñ∫ done
     ‚îú‚îÄ‚ñ∫ renta ‚îÄ‚ñ∫ done
     ‚îî‚îÄ‚ñ∫ mitma_ine_relations ‚îÄ‚ñ∫ done
```

### 3. `bronze_holidays` - Ingesta de festivos

**Schedule:** Manual (`schedule=None`)  
**Tags:** `bronze`, `holidays`, `data-ingestion`

**Par√°metros:**
| Param | Tipo | Descripci√≥n |
|-------|------|-------------|
| `year` | integer | A√±o de festivos |

### 4. `silver` - Transformaciones Silver Layer

**Schedule:** Dataset-triggered (espera a los 3 Bronze DAGs)  
**Tags:** `silver`, `data-transformation`

```python
schedule=[
    Dataset("bronze://mitma/done"), 
    Dataset("bronze://ine/done"), 
    Dataset("bronze://holidays/done")
]
```

**Tasks:**
```
start ‚îÄ‚ñ∫ mitma_ine_mapping ‚îÄ‚ñ∫ zonification ‚îÄ‚ñ∫ distances
                          ‚îî‚îÄ‚ñ∫ ine_group ‚îÄ‚ñ∫ silver_ine_all
     ‚îú‚îÄ‚ñ∫ overnight_stay ‚îÄ‚ñ∫ done
     ‚îú‚îÄ‚ñ∫ people_day ‚îÄ‚ñ∫ done
     ‚îî‚îÄ‚ñ∫ mitma_od_batches ‚îÄ‚ñ∫ od_quality_batches ‚îÄ‚ñ∫ done
```

### 5. `gold_gravity_model` - An√°lisis Modelo de Gravedad

**Schedule:** Manual (`schedule=None`)  
**Tags:** `gold`, `analytics`, `gravity-model`

**Par√°metros:**
| Param | Tipo | Descripci√≥n |
|-------|------|-------------|
| `start_date` | date | Inicio per√≠odo an√°lisis |
| `end_date` | date | Fin per√≠odo an√°lisis |

**Tasks:**
```
start ‚îÄ‚ñ∫ gravity_model ‚îÄ‚ñ∫ export ‚îÄ‚ñ∫ done
              ‚îÇ               ‚îÇ
              ‚îú‚îÄ create_base_pairs
              ‚îú‚îÄ calibrate_model
              ‚îú‚îÄ compute_mismatch
              ‚îî‚îÄ zone_ranking
                              ‚îÇ
                              ‚îú‚îÄ export_parquet
                              ‚îî‚îÄ export_keplergl
```

---

## üìä Tablas y Transformaciones

### Bronze Layer Tables

| Tabla | Fuente | Descripci√≥n |
|-------|--------|-------------|
| `bronze_mitma_od_municipios` | MITMA RSS | Matrices OD por municipio |
| `bronze_mitma_people_day_municipios` | MITMA RSS | Personas/d√≠a por zona |
| `bronze_mitma_overnight_stay_municipios` | MITMA RSS | Pernoctaciones |
| `bronze_mitma_municipios` | MITMA RSS | Zonificaci√≥n (geometr√≠as) |
| `bronze_ine_municipios` | INE API | Cat√°logo municipios |
| `bronze_ine_empresas_municipio` | INE API | Empresas por municipio |
| `bronze_ine_poblacion_municipio` | INE API | Poblaci√≥n por municipio |
| `bronze_ine_renta_municipio` | INE API | Renta por municipio |
| `bronze_mitma_ine_relations` | MITMA | Mapping MITMA ‚Üî INE |
| `bronze_spanish_holidays` | Python lib | Festivos nacionales |

### Silver Layer Tables

#### `silver_mitma_ine_mapping`
```mermaid
flowchart LR
  B1[bronze_ine_municipios] --> S[silver_mitma_ine_mapping]
  B2[bronze_mitma_ine_relations] --> S
```

**Transformaciones:**
- Normalizaci√≥n de nombres: `strip_accents` + `lower` + `trim`
- JOIN por c√≥digo INE
- Deduplicaci√≥n con `DISTINCT`
- Filtros de calidad (no nulls)

#### `silver_zones`
```mermaid
flowchart LR
  B[bronze_mitma_municipios] --> S[silver_zones]
  M[silver_mitma_ine_mapping] --> S
```

**Transformaciones:**
- Geometr√≠a: `ST_GeomFromText()` ‚Üí `ST_Multi()`
- C√°lculo de centroide: `ST_Centroid()`
- Filtro por cobertura de mapping

#### `silver_mitma_od`
```mermaid
flowchart LR
  B[bronze_mitma_od_municipios] --> S[silver_mitma_od]
  H[bronze_spanish_holidays] --> S
```

**Transformaciones:**
- Parse fecha/hora: `strptime(fecha || LPAD(periodo), '%Y%m%d%H')`
- Cast tipos: `viajes ‚Üí DOUBLE`
- Enriquecimiento: `is_weekend`, `is_holiday`
- Filtros: excluir `origen = 'externo'`
- Agregaci√≥n: `SUM(viajes) GROUP BY fecha, origen, destino, residencia`

**Particionado:**
```sql
PARTITIONED BY (year(fecha), month(fecha), day(fecha))
```

#### `silver_mitma_distances`
```mermaid
flowchart LR
  Z[silver_zones] --> D[silver_mitma_distances]
```

**Transformaciones:**
- Cross join de todas las zonas
- Distancia esf√©rica: `ST_Distance_Sphere(o.centroid, d.centroid) / 1000`
- Filtro pares √∫nicos: `WHERE o.id < d.id`

#### `silver_ine_empresas_municipio`
```mermaid
flowchart LR
  B[bronze_ine_empresas_municipio] --> S[silver_ine_empresas_municipio]
  M[silver_mitma_ine_mapping] --> S
```

**Transformaciones:**
- Explosi√≥n de arrays: `UNNEST(Data)`
- Normalizaci√≥n de nombres para matching
- Join fuzzy por nombre: `ILIKE`
- Agregaci√≥n: `SUM(valor) GROUP BY zone_id`

#### `silver_ine_poblacion_municipio`

**Transformaciones similares a empresas, con:**
- Pivot: `tipo = 'total' | 'hombres' | 'mujeres'`
- Columnas resultado: `poblacion_total`, `poblacion_hombres`, `poblacion_mujeres`

#### `silver_ine_renta_municipio`

**Transformaciones:**
- Filtro sem√°ntico: `tipo = 'renta neta media por persona'`
- Agregaci√≥n: `AVG(valor) ‚Üí renta_media`

#### `silver_ine_all`
```mermaid
flowchart LR
  Z[silver_zones] --> A[silver_ine_all]
  E[silver_ine_empresas_municipio] --> A
  P[silver_ine_poblacion_municipio] --> A
  R[silver_ine_renta_municipio] --> A
```

**Tabla consolidada** con todos los indicadores INE por zona MITMA.

---

## üîÑ Sistema de Tracking de Procesamiento

Para garantizar **idempotencia** y evitar reprocesamiento, se implementa un sistema de tracking mediante tablas intermedias.

### Tabla `silver_mitma_od_processed_dates`

```sql
CREATE TABLE silver_mitma_od_processed_dates (
    fecha VARCHAR  -- Formato: 'YYYYMMDD'
);
```

**Flujo:**

1. **Obtener fechas no procesadas:**
```sql
SELECT DISTINCT b.fecha
FROM bronze_mitma_od_municipios b
WHERE CAST(b.fecha AS VARCHAR) NOT IN (
    SELECT fecha FROM silver_mitma_od_processed_dates
)
```

2. **Procesar batch de fechas:**
```sql
INSERT INTO silver_mitma_od ...
WHERE fecha IN ('20230301', '20230302', ...)
```

3. **Registrar fechas procesadas:**
```sql
MERGE INTO silver_mitma_od_processed_dates AS target
USING (VALUES ('20230301'), ('20230302'), ...) AS source(fecha)
ON target.fecha = source.fecha
WHEN NOT MATCHED THEN INSERT (fecha) VALUES (source.fecha);
```

### Tracking en Bronze Layer

Para tablas Bronze, el tracking se hace mediante la columna `source_file` (CSV) o `source_url` (JSON):

```python
def _filter_csv_urls(table_name: str, urls: list[str]):
    """Filtra URLs ya ingestadas consultando source_file."""
    
    ingested_df = con.execute(f"""
        SELECT DISTINCT source_file 
        FROM {table_name}
        WHERE source_file IN ({url_list})
    """).fetchdf()
    
    ingested_urls = set(ingested_df['source_file'].tolist())
    new_urls = [url for url in urls if url not in ingested_urls]
    
    return new_urls
```

Esto permite:
- **Re-ejecutar DAGs** sin duplicar datos
- **Procesar incrementalmente** solo datos nuevos
- **Recuperaci√≥n de fallos** - continuar desde donde se qued√≥

---

## ‚òÅÔ∏è Google Cloud Run Jobs

Para procesar grandes vol√∫menes de datos, las tareas pesadas se ejecutan en **Google Cloud Run Jobs** con recursos dedicados (32GB RAM, 8 CPUs).

### Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                         AIRFLOW WORKER                               ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  1. Descarga archivo desde MITMA                                     ‚îÇ
‚îÇ  2. Sube archivo a RustFS (s3://mitma-raw/od/municipios/...)        ‚îÇ
‚îÇ  3. Invoca Cloud Run Job con par√°metros                              ‚îÇ
‚îÇ  4. Espera resultado (polling)                                       ‚îÇ
‚îÇ  5. Elimina archivo temporal de RustFS                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      CLOUD RUN JOB                                   ‚îÇ
‚îÇ                  (32GB RAM, 8 CPUs)                                  ‚îÇ
‚îÇ                                                                      ‚îÇ
‚îÇ  1. Conecta a DuckLake (PostgreSQL + RustFS)                         ‚îÇ
‚îÇ  2. Lee CSV desde RustFS S3                                          ‚îÇ
‚îÇ  3. Ejecuta MERGE INTO para insertar/actualizar                      ‚îÇ
‚îÇ  4. Cierra conexi√≥n y termina                                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Job: `ingestor_cloud` (MERGE CSV)

**Entrada (env vars):**
- `TABLE_NAME`: Nombre de la tabla destino (ej: `bronze_mitma_od_municipios`)
- `URL`: Ruta S3 del archivo (ej: `s3://mitma-raw/od/municipios/20230306_Viajes_municipios.csv.gz`)
- `ORIGINAL_URL`: URL original para auditor√≠a

**C√≥digo principal:**
```python
def main():
    table_name = os.environ.get("TABLE_NAME")
    s3_path = os.environ.get("URL")
    
    con = get_ducklake_connection(duckdb_config={
        'memory_limit': '28GB',
        'threads': 8,
        'worker_threads': 8,
    })
    
    # Leer CSV y hacer MERGE
    con.execute(f"""
        MERGE INTO {table_name} AS target
        USING (
            SELECT *, CURRENT_TIMESTAMP AS loaded_at, '{s3_path}' AS source_file
            FROM read_csv('{s3_path}', all_varchar=true)
        ) AS source
        ON {merge_condition}
        WHEN NOT MATCHED THEN INSERT *;
    """)
```

### Job: `executor_cloud` (SQL Executor)

**Entrada (env vars):**
- `SQL_QUERY`: Consulta SQL a ejecutar (puede ser multi-statement)

**Uso:**
```python
from utils.gcp import execute_sql_or_cloud_run

result = execute_sql_or_cloud_run(sql_query="""
    CREATE OR REPLACE TABLE silver_mitma_od AS ...
""")
```

### Fallback Local

Si Cloud Run no est√° configurado, las mismas operaciones se ejecutan localmente en el worker de Airflow:

```python
def execute_sql_or_cloud_run(sql_query: str, **context):
    if _get_cloud_run_connection():
        return exec_gcp_ducklake_executor(sql_query=sql_query)
    else:
        # Ejecutar localmente
        con = get_ducklake_connection()
        con.execute(sql_query)
```

---

## üìà Modelo de Gravedad (Gold Layer)

### Pregunta de Negocio

> **¬øD√≥nde est√° m√°s deficitaria la infraestructura de transporte?**

El modelo de gravedad estima la demanda potencial de viajes entre zonas bas√°ndose en:
- Poblaci√≥n en origen (genera viajes)
- Actividad econ√≥mica en destino (atrae viajes)
- Distancia (fricci√≥n)

### F√≥rmula del Modelo

```
T_ij = k √ó (P_i √ó E_j) / d_ij¬≤
```

Donde:
- `T_ij`: Viajes estimados entre zona i y zona j
- `P_i`: Poblaci√≥n en zona origen
- `E_j`: Empresas en zona destino
- `d_ij`: Distancia entre centroides (km)
- `k`: Constante de calibraci√≥n

### Tablas Gold

#### `gold_gravity_base_pairs`
Todos los pares O-D con sus caracter√≠sticas:
- Distancias entre centroides
- Poblaci√≥n origen/destino
- Empresas origen/destino
- Renta media origen/destino
- Viajes reales agregados
- D√≠as observados

#### `gold_gravity_calibrated`
Modelo calibrado con constante k:
```sql
k = SUM(viajes_reales) / SUM(T_raw)
T_estimado = k √ó (P_i √ó E_j) / d_ij¬≤
```

#### `gold_gravity_mismatch`
Ratio de desajuste por par O-D:
```sql
mismatch_ratio = T_estimado / (viajes_reales + 1)
unmet_demand = T_estimado - viajes_reales
```

- `mismatch_ratio > 1`: Demanda potencial mayor que real ‚Üí **d√©ficit de infraestructura**
- `mismatch_ratio < 1`: Demanda real mayor que estimada ‚Üí infraestructura suficiente

#### `gold_gravity_zone_ranking`
M√©tricas agregadas por zona:
```sql
SELECT 
    zone_id,
    AVG(mismatch_ratio) AS avg_mismatch,
    SUM(unmet_demand) AS total_unmet_demand,
    COUNT(*) AS num_corridors_deficit
GROUP BY zone_id
ORDER BY total_unmet_demand DESC
```

### Outputs para Visualizaci√≥n

| Archivo | Formato | Descripci√≥n |
|---------|---------|-------------|
| `keplergl_dashboard_*.html` | HTML | Dashboard interactivo con mapas |
| `zones_*.geojson` | GeoJSON | Pol√≠gonos de zonas coloreados por d√©ficit |
| `corridors_*.geojson` | GeoJSON | L√≠neas O-D con grosor por demanda |
| `*.parquet` | Parquet | Datos raw para BI tools |

---

## üö¶ Ejecuci√≥n del Pipeline

### 1. Ingesta Bronze (en paralelo)

```bash
# Trigger MITMA para marzo 2023
airflow dags trigger bronze_mitma \
  --conf '{"start": "2023-03-01", "end": "2023-03-31", "enable_people_day": false, "enable_overnight": false}'

# Trigger INE para 2023
airflow dags trigger bronze_ine \
  --conf '{"year": "2023"}'

# Trigger Holidays para 2023
airflow dags trigger bronze_holidays \
  --conf '{"year": 2023}'
```

### 2. Transformaci√≥n Silver

Se dispara autom√°ticamente cuando los 3 Bronze DAGs completan (Airflow Datasets).

### 3. An√°lisis Gold

```bash
airflow dags trigger gold_gravity_model \
  --conf '{"start_date": "2023-03-01", "end_date": "2023-03-31"}'
```

---

## üêõ Bugs Conocidos

### DuckLake + MERGE INTO + Particionado con funciones

**Problema:** Al usar `MERGE INTO` con tablas particionadas por funciones `year(fecha)`, `month(fecha)`, `day(fecha)`, se generan valores de partici√≥n incorrectos.

**Soluci√≥n:** Usar `INSERT INTO` en lugar de `MERGE INTO` para `silver_mitma_od`. El tracking de fechas procesadas garantiza idempotencia.

```sql
-- ‚úÖ FUNCIONA
INSERT INTO silver_mitma_od ...
ALTER TABLE silver_mitma_od SET PARTITIONED BY (year(fecha), month(fecha), day(fecha));

-- ‚ùå NO FUNCIONA (genera particiones corruptas)
MERGE INTO silver_mitma_od ... 
-- Produce: year=-1977503543602676920 en lugar de year=2023
```

---

## üìÅ Estructura del Proyecto

```
airflow/
‚îú‚îÄ‚îÄ dags/
‚îÇ   ‚îú‚îÄ‚îÄ bronze/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bronze_mitma_dag.py      # DAG principal MITMA
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bronze_ine_dag.py        # DAG principal INE
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bronze_holidays_dag.py   # DAG festivos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tasks/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mitma/               # Tasks MITMA (OD, People, Overnight, Zonification)
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ine/                 # Tasks INE (Municipios, Empresas, Poblaci√≥n, Renta)
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ holidays/            # Tasks festivos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ utils.py                 # Utilidades Bronze (URLs, merge, filter)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ silver/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ silver_dag.py            # DAG principal Silver
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mitma/                   # Tasks Silver MITMA
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ine/                     # Tasks Silver INE
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ misc/                    # Quality checks, cleanup
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ gold/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gold_gravity_model_dag.py  # DAG modelo de gravedad
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tasks/gravity_model/       # Tasks del modelo
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ misc/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ infra.py                 # TaskGroup infraestructura
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ utils.py                 # DuckLake connection manager
‚îÇ       ‚îî‚îÄ‚îÄ gcp.py                   # Google Cloud Run utilities
‚îÇ
‚îú‚îÄ‚îÄ gcp/
‚îÇ   ‚îú‚îÄ‚îÄ ingestor_cloud/              # Cloud Run Job para MERGE CSV
‚îÇ   ‚îî‚îÄ‚îÄ executor_cloud/              # Cloud Run Job para SQL
‚îÇ
‚îú‚îÄ‚îÄ docs/
‚îÇ   ‚îî‚îÄ‚îÄ bronze_to_silver_transformations.md  # Documentaci√≥n transformaciones
‚îÇ
‚îî‚îÄ‚îÄ requirements.txt                 # Dependencias Python
```

---

## üìù Licencia

Este proyecto fue desarrollado como parte del M√°ster en Ciencia e Ingenier√≠a de Datos (MUCEIM) - Universidad Polit√©cnica de Valencia.

